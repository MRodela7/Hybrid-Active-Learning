# -*- coding: utf-8 -*-
"""Automation Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uL2mDxZwGqBHCvetYskoczpInePHAwGB
"""

# Pre-processing of data, create dataset

import pandas as pd

# Load data
SBS_df = pd.read_csv('PCAWG_sigProfiler_SBS_signatures_in_samples.csv')
DBS_df = pd.read_csv('PCAWG_sigProfiler_DBS_signatures_in_samples.csv')
ID_df = pd.read_csv('PCAWG_sigProfiler_ID_signatures_in_samples.csv')
age_df = pd.read_excel('PCAWG7_age_information.xlsx')

# Merge dataframes
SBS_ID_merged = pd.merge(SBS_df, ID_df, on = 'Sample Names')
all_merged = pd.merge(SBS_ID_merged, DBS_df, on = 'Sample Names', how = 'outer')
all_merged.fillna('N/A', inplace = True)
all_merged = all_merged.drop(columns = ['Accuracy_x', 'Cancer Types_y', 'Accuracy_y', 'Cancer Types', ' Accuracy'])
all_merged = all_merged.rename(columns = {'Cancer Types_x': 'Cancer Types'})
all_merged = all_merged.reset_index(drop = True)

# Add age column
ages = []
for index, row in all_merged.iterrows():
    sampleName = row['Sample Names']
    hasAge = False
    for index2, row2 in age_df.iterrows():
        if row2['Sample Name'] == sampleName:
            age = row2['Age of diagnosis']
            hasAge = True
            break
    if hasAge:
        ages.append(age)
    else:
        ages.append('N/A')
all_merged['Age'] = ages

# Drop samples with N/A values
all_merged = all_merged.replace('N/A', np.nan).dropna(axis = 0, how = 'any')

# Save as file
all_merged.to_excel('PCAWG_merged.xlsx')

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path_PCWG_rodela = '/content/drive/MyDrive/Colab Notebooks/PCAWG_merged.xlsx'
path_PCWG_derek = '/content/drive/MyDrive/Automation in Scientific Research/PCAWG_merged.xlsx'
path_PCAWG_rajdeep = '/content/drive/MyDrive/PCAWG_merged.xlsx'

# Read excel file of the merged dataset into a dataframe
# df_pcwg = pd.read_excel(path_PCWG_rodela)
# df_pcwg = pd.read_excel(path_PCWG_derek)
df_pcwg = pd.read_excel(path_PCAWG_rajdeep)
df_pcwg

# Create the set of x features and y features

import copy
import numpy as np

# Cancer types (labels)
y = np.array(df_pcwg.iloc[:, 1].values)
label_codes = np.unique(y, return_inverse=True)[1]
display(y)

# Input (SBSs)
X = []
columns = []
for i in range(66):
  columns.append(np.array(df_pcwg.iloc[:, i + 3].values))
for i in range(columns[0].shape[0]):
  rowList = []
  for j in range(66):
    rowList.append(columns[j][i])
  X.append(rowList)
X = np.array(X)

print(X.shape)
print(label_codes.shape)
print("X:", X.shape)

# Visualize preliminary dataset using PCA

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

# Convert labels to numeric codes
label_codes = np.unique(y, return_inverse=True)[1]
print("label_codes:", label_codes)

# Scale features using StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Perform PCA to reduce dimensionality to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Define a color map
cmap = plt.get_cmap('rainbow', len(np.unique(y)))

# Create scatter plot
fig, ax = plt.subplots()
scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=label_codes, cmap=cmap)

# Add colorbar legend
legend = ax.legend(*scatter.legend_elements(), loc='lower right', title='Classes')
ax.add_artist(legend)

# Add axis labels and title
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Scatter Plot with Color-Coding')

# Show plot
plt.show()

import pandas as pd 
import numpy as np
import copy, random
from logging import exception
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import LeaveOneOut
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from numpy import mean
from numpy import absolute
from numpy import sqrt
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans

# Uncertainty Sampling

def get_square(xtrain, ytrain, model):
  y_prob = model.predict(xtrain)
  loss = []
  n = len(xtrain)
  for i in range(len(ytrain)):
    val = (ytrain[i]-y_prob[i])**2
    loss.append(val)
  return sum(loss)/(n-2)

def getSx(xtrain):
  val = (np.sum(xtrain))**2/(len(xtrain))
  res = np.sum((xtrain)**2)
  return res-val

def calculate_variance(x_prime, xtrain, ytrain, model):
  n = len(xtrain)
  s_square = get_square(xtrain, ytrain, model)
  
  sxfeat1 = getSx(xtrain[:, 0])
  sxfeat2 = getSx(xtrain[:, 1])

  val1 = s_square* (1/n + ( ( (x_prime[0]-(np.mean(xtrain[:, 0])))**2) /sxfeat1))
  val2 = s_square*((1/n)+(((x_prime[1]-(np.mean(xtrain[:, 1])))**2)/sxfeat2))
  return val1+val2

def getRemains(x, y, n):
  random_indices_10 = np.random.choice(x.shape[0], n, replace=False)
  x_train_10 = x[random_indices_10, :]
  y_train_10 = y[random_indices_10]
  remain_x_10 = np.delete(x, random_indices_10, axis=0)
  remain_y_10 = np.delete(y, random_indices_10, axis = 0)
  return x_train_10, y_train_10.flatten(), remain_x_10, remain_y_10.flatten()

def uncertaintySampling(xtrain, ytrain, remain_x, num):
    vList = []
    model1 = RandomForestClassifier().fit(xtrain, ytrain)
    for x_prime in remain_x:
      variance = calculate_variance(x_prime, xtrain, ytrain, model1)
      vList.append(variance)
    sorted_array = np.argsort(vList)
    idx = sorted_array[-num:]
    return idx

# Get classification accuracy (CV score) of input
def get_accuracy(X_train, y_train):
  xtrain, ytrain, xtest, ytest = train_test_split(X_train, y_train)
  model = RandomForestClassifier()
  model.fit(X_train, y_train)
  scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
  scoreMean = np.mean(scoreA)
  return scoreMean

#Query Based Sampling

def query_sampling(remX, remY, committee_size, num):
    # Initialize a committee of models
    committee = []
    if len(remX)== 2: return 0
    # Do bootstrapping selection of committee
    for i in range(committee_size):
        model = RandomForestClassifier()
        model.fit(remX, remY)
        committee.append(model)
         
    # Predict the labels of the unlabeled data using the committee
    predictions = np.zeros((remX.shape[0], committee_size))
    for i, model in enumerate(committee):
        if len(remX)== 2: return 0
        predictions[:, i] = model.predict_proba(remX)[:, 1]

    # Calculate the disagreement or uncertainty of the committee's predictions
    # Use hard vote, soft vote
    disagreement = np.sum(predictions * np.log2(predictions) + (1 - predictions) * np.log2(1 - predictions), axis=1) / committee_size
    if len(disagreement) == 1: return 0
    # Select the unlabeled example with the highest disagreement score
    index = np.argsort(disagreement)[-num:]
    return index

# Density Sampling

def query_sampling2(remX, remY, committee_size, num):
    # Initialize a committee of models
    committee = []
    # if len(remX)== 1: return 0
    # Do bootstrapping selection of committee
    for i in range(committee_size):
        model = RandomForestClassifier()
        model.fit(remX, remY)
        committee.append(model)
         
    # Predict the labels of the unlabeled data using the committee
    predictions = np.zeros((remX.shape[0], committee_size))
    for i, model in enumerate(committee):
        predictions[:, i] = model.predict_proba(remX)[:, 1]

    # Calculate the disagreement or uncertainty of the committee's predictions
    # Use hard vote, soft vote
    disagreement = np.sum(predictions * np.log2(predictions) + (1 - predictions) * np.log2(1 - predictions), axis=1) / committee_size
    if len(disagreement) == 1: return 0
    # Select the unlabeled example with the highest disagreement score
    index = np.argsort(disagreement)[:num]
    return index, disagreement

def density_sampling(remain_x, remain_y, xtrain, num, k=1, eps=1e-4):
    # Estimated using k-nearest neighbors, points with density below eps are considered low-density
    
    if len(remain_x) == 1: return 0
    density_val = []
    # Calculate the k-nearest neighbors for each point in X
    nbrs = NearestNeighbors(n_neighbors=k+1).fit(xtrain)
    total = 0
    for x_prime in remain_x:
      x_prime = np.reshape(x_prime, (1, 2))
      distances, indices = nbrs.kneighbors(x_prime)
      # Calculate the density of each point based on the distance to its k-th neighbor
      density = 1.0 / (distances[:,k] + 1e-6)
      total += density
    total = total/len(remain_x)

    query_index, disagreem = query_sampling2(remain_x, remain_y, num, committee_size)
    val = disagreem*total
    density_val.append(val)
    density_val = np.array(density_val)
    
    return np.argsort(density_val)[:num]

pip install scikit-learn-extra

# Diversity Sampling

from sklearn_extra.cluster import KMedoids

def diversity_sampling(X, num):
    n_samples = num
    # Step 1: Cluster the data using k-medoids
    if num>X.shape[0]: n_samples = X.shape[0]
    kmedoids = KMedoids(n_clusters=n_samples, metric='euclidean')
    kmedoids.fit(X)
    
    # Step 2: Select the most representative samples from each cluster
    indices_sampled = []
    for cluster_id in range(n_samples):
        cluster_indices = np.where(kmedoids.labels_ == cluster_id)[0]
        cluster_data = X[cluster_indices]
        centroid_index = kmedoids.medoid_indices_[cluster_id]
        centroid_data = X[centroid_index]
        dist_to_centroid = np.linalg.norm(cluster_data - centroid_data, axis=1)
        # Select the top samples based on their distance to the medoid
        top_samples_indices = cluster_indices[np.argsort(dist_to_centroid)][:1]
        # print("top_samples_indices:", top_samples_indices.shape)
        indices_sampled.extend(top_samples_indices)
    
    return indices_sampled

# Active Learning Hybrid

from sklearn.model_selection import train_test_split

def activeLearning_hybrid(X_train, y_train, remain_x, remain_y):
  scoreMeanList_ob = []
  scoreList_un = []
  committee_size = 5
  i = 0
  accuracy_matrix = []
  proportion = [0.33, 0.33, 0.34]
  xtrain_u = copy.deepcopy(X_train)
  xtrain_q = copy.deepcopy(X_train)
  xtrain_d = copy.deepcopy(X_train)
  ytrain_u = copy.deepcopy(y_train)
  ytrain_q = copy.deepcopy(y_train)
  ytrain_d = copy.deepcopy(y_train)
  batch_size = 100

  while remain_x.shape[0] >= 2:
    num_u = int(proportion[0]*batch_size)
    num_q = int(proportion[1]*batch_size)
    num_d = int(batch_size - num_q-num_u)

    if num_u+num_q + num_d != batch_size: 
      print(num_u+num_q + num_d)
      raise exception
    remain_x_copy = copy.deepcopy(remain_x)
    remain_y_copy = copy.deepcopy(remain_y)

    # Uncertainty sampling
    if remain_x_copy.shape[0] <= 2: break
    selected_indices_u = uncertaintySampling(X_train, y_train, remain_x_copy, num_u)
    
    # Query by committee
    if remain_x_copy.shape[0] <= 2: break
    selected_indices_q = query_sampling(remain_x_copy, remain_y_copy, committee_size, num_q)

    # Diversity sampling
    if remain_x_copy.shape[0] <= 2: break
    selected_indices_d = diversity_sampling(remain_x_copy, num_d)

    merged_selected_indices =  set(list(selected_indices_u) + list(selected_indices_q) + list(selected_indices_d))

    # x train for calculating individual accuracy
    xtrain_u = np.concatenate((xtrain_u, remain_x[selected_indices_u]))
    xtrain_q = np.concatenate((xtrain_q, remain_x[selected_indices_q]))
    xtrain_d = np.concatenate((xtrain_d, remain_x[selected_indices_d]))

    # y train for calculating individual accuracy
    ytrain_u = np.concatenate((ytrain_u, remain_y[selected_indices_u]))
    ytrain_q = np.concatenate((ytrain_q, remain_y[selected_indices_q]))
    ytrain_d = np.concatenate((ytrain_d, remain_y[selected_indices_d]))
   
    merged_selected_indices = list(merged_selected_indices)

    # Update train sets (combined sampling methods)
    X_train = np.concatenate((X_train, remain_x[merged_selected_indices]))
    y_train = np.concatenate((y_train, remain_y[merged_selected_indices]))

    # Update unobserved sets (combined sampling methods)
    remain_x = np.delete(remain_x, merged_selected_indices, axis=0)
    remain_y = np.delete(remain_y, merged_selected_indices, axis=0)

    accuracy_u = get_accuracy(xtrain_u, ytrain_u)
    accuracy_q = get_accuracy(xtrain_q, ytrain_q)
    accuracy_d = get_accuracy(xtrain_d, ytrain_d)

    # Update proportion
    sum_acc = accuracy_u + accuracy_q + accuracy_d
    accuracy_matrix.append([accuracy_u, accuracy_q, accuracy_d, sum_acc])

    numerator_u = 0
    numerator_q = 0
    numerator_d = 0
    denominator = 0

    for i in range(len(accuracy_matrix)):
      numerator_u += accuracy_matrix[i][0]
      numerator_q += accuracy_matrix[i][1]
      numerator_d += accuracy_matrix[i][2]
      denominator += accuracy_matrix[i][3]

    proportion[0] = numerator_u / denominator
    proportion[1] = numerator_q / denominator
    proportion[2] = numerator_d / denominator
    
    # Train Model with the sampled data
    if remain_x.shape[0] <= 2: break
    
    model = RandomForestClassifier()
    model.fit(X_train, y_train)
    scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    scoreMean = np.mean(scoreA)
    scoreMeanList_ob.append(scoreMean) 

    # if remain_x.shape[0] == 1 : break
    prob = model.score(remain_x, remain_y) # Gives the score accuracy
    scoreList_un.append(prob)

    print(scoreMeanList_ob)
    print(scoreList_un)

  return scoreMeanList_ob, scoreList_un

observedList,  unobservedList = [], []
for i in range(3, 6):
  print("simulation number:", i)
  np.random.seed(123+i)

  copyY = copy.deepcopy(label_codes)
  copyX = copy.deepcopy(X)

  random_indices = np.random.choice(copyX.shape[0], 664, replace=False) 
  random_10_x = copyX[random_indices, :]
  random_10_y = copyY[random_indices]
  
  X_train = random_10_x
  y_train = random_10_y
  
  remain_x = np.delete(copyX, random_indices, axis=0)
  remain_y = np.delete(copyY, random_indices, axis=0)
  
  scoreMeanList_ob, scoreList_un = activeLearning_hybrid(X_train, y_train, remain_x, remain_y)

  observedList.append(scoreMeanList_ob)
  unobservedList.append(scoreList_un)

print("obList:",np.array(observedList).shape)
print("Un obList:",np.array(unobservedList).shape)

print("obList:",np.array(observedList_un).shape)
print("Un obList:",np.array(unobservedList_un).shape)

x1 = np.arange(np.array(observedList_un).shape[1])
# print(np.array(observedList)
x2 = np.arange(np.array(unobservedList_un).shape[1])


y1 = np.mean(np.array(observedList_un), axis = 0)
y1_std = np.std(observedList_un, axis = 0)
# print("y1:", y1.shape)

y2 = np.mean(np.array(unobservedList_un), axis = 0)
y2_std = np.std(unobservedList_un, axis = 0)


plt.plot(x2, y2, label = "observed")
plt.errorbar(x2, y2, yerr=y2_std, label='Un Observed Data', fmt ='o', color='black', ecolor='blue', capsize=5)
# plt.plot(x2, y2, label ="unobserved")
# plt.errorbar(x2, y2, yerr=y2_std, label='Unobserved Data', fmt = 'x', color='red', ecolor='red', capsize=5)
plt.xlabel('Data')
plt.ylabel('Accuracy')
plt.title('Active Learning by Hybrid Sampling Methods')
plt.legend()
plt.show()



"""Plotting the 5 graphs: Query, Uncertainity, Diversity."""

# Looking at the individual sampling methods:

# Query by Committee Sampling
def query_sampling_indi(remX, remY, committee_size):
    # Initialize a committee of models
    committee = []
    # if len(remX)== 1: return 0
    # Do bootstrapping selection of committee
    for i in range(committee_size):
        model = RandomForestClassifier()
        model.fit(remX, remY)
        committee.append(model)
         
    # Predict the labels of the unlabeled data using the committee
    predictions = np.zeros((remX.shape[0], committee_size))
    for i, model in enumerate(committee):
        predictions[:, i] = model.predict_proba(remX)[:, 1]

    # Calculate the disagreement or uncertainty of the committee's predictions
    # Use hard vote, soft vote
    disagreement = np.sum(predictions * np.log2(predictions) + (1 - predictions) * np.log2(1 - predictions), axis=1) / committee_size
    if len(disagreement) == 1: return 0
    # Select the unlabeled example with the highest disagreement score
    sorted_array = np.argsort(disagreement)
    index = sorted_array[-100:]
    return index

def activeLearning_by_query(xtrain, ytrain, rem_x, rem_y, committee_size):
  ob, unob =[], []
  length = 0
  model = RandomForestClassifier()
  while rem_x.shape[0] >=2:
    if rem_x.shape[0] <=2: break
    queried_index = query_sampling_indi(rem_x, rem_y, committee_size)

    xtrain = np.vstack((xtrain, rem_x[queried_index]))
    ytrain = np.hstack((ytrain, rem_y[queried_index]))
      
    rem_x = np.delete(rem_x, queried_index, axis=0)
    rem_y = (np.delete(rem_y, queried_index, axis=0)).flatten()
    
    # Iterate over the folds
    if rem_x.shape[0] <= 2: break
    model.fit(X_train, y_train)
    scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring = 'accuracy')
    scoreMean = np.mean(scoreA)

    ob.append(scoreMean) # as we increment the unobsereved data with 1
    unob_score = model.score(rem_x, rem_y)
    length = xtrain.shape[0]
    unob.append(unob_score)

    print(ob)
    print(unob)
  return ob, unob

observedList_q, unobservedList_q = [], []
committee_size = 5

for i in range(5):
  print("simulation no:", i)
  np.random.seed(123+i)

  copyY = copy.deepcopy(label_codes)
  copyX = copy.deepcopy(X)

  # Simulation starts with 20 random data points
  random_indices = np.random.choice(copyX.shape[0], 664, replace=False) 
  random_20_x = copyX[random_indices, :]
  random_20_y = copyY[random_indices]
  X_train = np.vstack(random_20_x)
  y_train = np.hstack(random_20_y).flatten()
  remain_x = np.delete(copyX, random_indices, axis=0)
  remain_y = (np.delete(copyY, random_indices, axis=0)).flatten()
  observed, unobserved = activeLearning_by_query(X_train, y_train, remain_x, remain_y, committee_size)
  observedList_q.append(observed)
  unobservedList_q.append(unobserved)

print("observedList:", np.array(observedList_q).shape)
print("unobservedList:", np.array(unobservedList_q).shape)

# Uncertainity sampling individual method
def uncertainitySampling_indi(xtrain, ytrain, remain_x, model):
  vList = []
  for x_prime in remain_x:
    variance = calculate_variance(x_prime, xtrain, ytrain, model)
    vList.append(variance)
  sorted_array = np.argsort(vList)
  idx = sorted_array[-100:]
  return idx

def get_square(xtrain, ytrain, model):
  y_prob = model.predict(xtrain)
  loss = []
  n = len(xtrain)
  for i in range(len(ytrain)):
    val = (ytrain[i]-y_prob[i])**2
    loss.append(val)
  return sum(loss)/(n-2)

def getSx(xtrain):
  val = (np.sum(xtrain))**2/(len(xtrain))
  res = np.sum((xtrain)**2)
  return res-val

def calculate_variance(x_prime, xtrain, ytrain, model):
  n = len(xtrain)
  s_square = get_square(xtrain, ytrain, model)
  
  sxfeat1 = getSx(xtrain[:, 0])
  sxfeat2 = getSx(xtrain[:, 1])

  val1 = s_square* (1/n + ( ( (x_prime[0]-(np.mean(xtrain[:, 0])))**2) /sxfeat1))
  val2 = s_square*((1/n)+(((x_prime[1]-(np.mean(xtrain[:, 1])))**2)/sxfeat2))
  return val1+val2

from sklearn.model_selection import train_test_split

def activeLearning_us(X_train, y_train, remain_x, remain_y):
  scoreMeanList_ob =[]
  scoreMeanList_un = []
  length = X_train.shape[0]
  score2List = []
  model = RandomForestClassifier().fit(X_train, y_train)
  
  while remain_x.shape[0] >=2:
    selected_indices = uncertainitySampling_indi(X_train, y_train, remain_x, model)

    X_train = np.vstack((X_train, remain_x[selected_indices]))
    y_train = np.hstack((y_train, remain_y[selected_indices]))
    
    remain_x = np.delete(remain_x, selected_indices, axis=0)
    remain_y = np.delete(remain_y, selected_indices, axis=0)

    print(len(remain_x))
    print(len(selected_indices))
   
    model = RandomForestClassifier()

    #Train model with the uncertainty sampled data
    if remain_x.shape[0] == 0: break
    model.fit(X_train, y_train)
    scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    scoreMean = np.mean(scoreA)

    scoreMeanList_ob.append(scoreMean) # as we increment the unobsereved data with 1
  
    length = X_train.shape[0]

    sc = model.score(remain_x, remain_y) # Gives the score accuracy
    scoreMeanList_un.append(sc)

    print(scoreMeanList_ob)
    print(scoreMeanList_un)
    
  return scoreMeanList_ob, scoreMeanList_un

observedList_us, unobservedList_us = [], []

for i in range(5):
  print("simulation no>", i)
  np.random.seed(123+i)

  copyY = copy.deepcopy(label_codes)
  copyX = copy.deepcopy(X)

  random_indices = np.random.choice(copyX.shape[0], 664, replace=False) 
  random_10_x = copyX[random_indices, :]
  random_10_y = copyY[random_indices]

  X_train = random_10_x
  y_train = random_10_y
  
  remain_x = np.delete(copyX, random_indices, axis=0)
  remain_y = np.delete(copyY, random_indices, axis=0)

  scoreMeanList_ob, scoreList_un = activeLearning_us(X_train, y_train, remain_x, remain_y)

  print(scoreMeanList_ob)
  print(scoreList_un)
 
  observedList_us.append(scoreMeanList_ob)
  unobservedList_us.append(scoreList_un)

print("obList:",np.array(observedList_us).shape)
print("Un obList:",np.array(unobservedList_us).shape)

!pip install cudf==0.19.2

# Diversity sampling individual method

from sklearn_extra.cluster import KMedoids

def diversity_sampling_indi(X, num):
    n_samples = num
    # Step 1: Cluster the data using k-medoids
    if num>X.shape[0]: n_samples = X.shape[0]
    kmedoids = KMedoids(n_clusters=n_samples, metric='euclidean')
    kmedoids.fit(X)
    
    # Step 2: Select the most representative samples from each cluster
    indices_sampled = []
    for cluster_id in range(n_samples):
        cluster_indices = np.where(kmedoids.labels_ == cluster_id)[0]
        cluster_data = X[cluster_indices]
        centroid_index = kmedoids.medoid_indices_[cluster_id]
        centroid_data = X[centroid_index]
        dist_to_centroid = np.linalg.norm(cluster_data - centroid_data, axis=1)
        # Select the top samples based on their distance to the medoid
        top_samples_indices = cluster_indices[np.argsort(dist_to_centroid)][:1]
        indices_sampled.extend(top_samples_indices)
    
    return indices_sampled

def activeLearning_diversity(X_train, y_train, remain_x, remain_y):
  scoreMeanList_ob = []
  scoreList_un = []
  length = X_train.shape[0]
  score2List = []
  
  n_samples = 100
  while remain_x.shape[0] >=2:
    selected_indices = diversity_sampling_indi(remain_x, n_samples)
  
    for idx1 in selected_indices:
      X_train = np.vstack((X_train, remain_x[idx1]))
      y_train = np.hstack((y_train, remain_y[idx1]))
  
    remain_x = np.delete(remain_x, selected_indices, axis=0)
    remain_y = np.delete(remain_y, selected_indices, axis=0)

    print(len(remain_x))
    print(selected_indices)
    print(len(selected_indices))
    
    model = RandomForestClassifier()

    # Train model with the diversity sampled data
    if remain_x.shape[0] <=2: break
    model.fit(X_train, y_train)
    scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    scoreMean = np.mean(scoreA)

    scoreMeanList_ob.append(scoreMean) # as we increment the unobsereved data with 1
    
    prob = model.score(remain_x, remain_y) # Gives the score accuracy
    scoreList_un.append(prob)

    print(scoreMeanList_ob)
    print(scoreList_un)

  return scoreMeanList_ob, scoreList_un

observedList_un_diversity,  unobservedList_un_diversity = [], []

for i in range(5):
  print("simulation no>", i)
  np.random.seed(123+i)

  copyY = copy.deepcopy(label_codes)
  copyX = copy.deepcopy(X)

  random_indices = np.random.choice(copyX.shape[0], 664, replace=False) 
  random_10_x = copyX[random_indices, :]
  random_10_y = copyY[random_indices]
  X_train = random_10_x
  y_train = random_10_y

  remain_x = np.delete(copyX, random_indices, axis=0)
  remain_y = np.delete(copyY, random_indices, axis=0)

  scoreMeanList_ob, scoreList_un = activeLearning_diversity(X_train, y_train, remain_x, remain_y)

  print(scoreMeanList_ob)
  print(scoreList_un)

  observedList_un_diversity.append(scoreMeanList_ob)
  unobservedList_un_diversity.append(scoreList_un)
  
print("obList:",np.array(observedList_un_diversity).shape)
print("Un obList:",np.array(unobservedList_un_diversity).shape)

from sklearn.model_selection import train_test_split

# Hybrid algorithm with homework dataset

def activeLearning_hybrid_small(X_train, y_train, remain_x, remain_y):
  scoreMeanList_ob = []
  scoreList_un = []
  committee_size = 5
  i = 0
  accuracy_matrix = []
  proportion = [0.33, 0.33, 0.34]
  xtrain_u = copy.deepcopy(X_train)
  xtrain_q = copy.deepcopy(X_train)
  xtrain_d = copy.deepcopy(X_train)
  ytrain_u = copy.deepcopy(y_train)
  ytrain_q = copy.deepcopy(y_train)
  ytrain_d = copy.deepcopy(y_train)
  batch_size = 10

  while remain_x.shape[0] >= 2:
    num_u = int(proportion[0]*batch_size)
    num_q = int(proportion[1]*batch_size)
    num_d = int(batch_size - num_q-num_u)

    if num_u+num_q + num_d != batch_size: 
      print(num_u+num_q + num_d)
      raise exception
    remain_x_copy = copy.deepcopy(remain_x)
    remain_y_copy = copy.deepcopy(remain_y)

    # Uncertainty sampling
    selected_indices_u = uncertaintySampling(X_train, y_train, remain_x_copy, num_u)
    
    # Query by committee
    selected_indices_q = query_sampling(remain_x_copy, remain_y_copy, committee_size, num_q)

    # Diversity sampling
    selected_indices_d = diversity_sampling(remain_x_copy, num_d)

    merged_selected_indices =  set(list(selected_indices_u) + list(selected_indices_q) + list(selected_indices_d))

    print(len(merged_selected_indices))
    print(len(X_train))
    print(len(remain_x))

    # x train for calculating individual accuracy
    xtrain_u = np.concatenate((xtrain_u, remain_x[selected_indices_u]))
    xtrain_q = np.concatenate((xtrain_q, remain_x[selected_indices_q]))
    xtrain_d = np.concatenate((xtrain_d, remain_x[selected_indices_d]))

    # y train for calculating individual accuracy
    ytrain_u = np.concatenate((ytrain_u, remain_y[selected_indices_u]))
    ytrain_q = np.concatenate((ytrain_q, remain_y[selected_indices_q]))
    ytrain_d = np.concatenate((ytrain_d, remain_y[selected_indices_d]))
   
    merged_selected_indices = list(merged_selected_indices)

    # Update train sets (combined sampling methods)
    X_train = np.concatenate((X_train, remain_x[merged_selected_indices]))
    y_train = np.concatenate((y_train, remain_y[merged_selected_indices]))

    # Update unobserved sets (combined sampling methods)
    remain_x = np.delete(remain_x, merged_selected_indices, axis=0)
    remain_y = np.delete(remain_y, merged_selected_indices, axis=0)

    accuracy_u = get_accuracy(xtrain_u, ytrain_u)
    accuracy_q = get_accuracy(xtrain_q, ytrain_q)
    accuracy_d = get_accuracy(xtrain_d, ytrain_d)

    # Update proportion
    sum_acc = accuracy_u + accuracy_q + accuracy_d
    accuracy_matrix.append([accuracy_u, accuracy_q, accuracy_d, sum_acc])

    numerator_u = 0
    numerator_q = 0
    numerator_d = 0
    denominator = 0

    for i in range(len(accuracy_matrix)):
      numerator_u += accuracy_matrix[i][0]
      numerator_q += accuracy_matrix[i][1]
      numerator_d += accuracy_matrix[i][2]
      denominator += accuracy_matrix[i][3]

    proportion[0] = numerator_u / denominator
    proportion[1] = numerator_q / denominator
    proportion[2] = numerator_d / denominator
    
    # Train model with the sampled data
    if remain_x.shape[0] <= 2: break
    
    model = RandomForestClassifier()
    model.fit(X_train, y_train)
    scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    scoreMean = np.mean(scoreA)
    scoreMeanList_ob.append(scoreMean) 

    # if remain_x.shape[0] == 1 : break
    prob = model.score(remain_x, remain_y) #gives the score accuracy
    scoreList_un.append(prob)

    print(scoreMeanList_ob)
    print(scoreList_un)

  return scoreMeanList_ob, scoreList_un

# path3 = '/content/drive/MyDrive/classification2.csv'
path3 = '/content/drive/MyDrive/Automation in Scientific Research/classification2.csv'

df3 = pd.read_csv(path3)
df3 = df3.drop(columns=df3.columns[-1])
#extract cost
cost = np.array(df3.iloc[:, -1])

#extract y
y3 = df3.iloc[:, 2]
y3 = ((np.array(y3))).flatten()

#extract x
x_class =[]
x1_c = np.array(df3.iloc[:,0].values)
x2_c = np.array(df3.iloc[:,1].values)
for i in range(x1_c.shape[0]):
  x_class.append([x1_c[i], x2_c[i]])
x_class = np.array(x_class)
copyX_class = copy.deepcopy(x_class)
copyY_class = copy.deepcopy(y3)
copy_cost = copy.deepcopy(cost)
print(copyX_class.shape)
print(copyY_class.shape)
print(cost.shape)

observedList_un_small,  unobservedList_un_small = [], []
for i in range(5):
  print("simulation number:", i)
  np.random.seed(123+i)

  copyX_class = copy.deepcopy(x_class)
  copyY_class = copy.deepcopy(copyY_class)

  random_indices = np.random.choice(copyX_class.shape[0], 60, replace=False) 
  random_10_x = copyX_class[random_indices, :]
  random_10_y = copyY_class[random_indices]
  
  X_train = random_10_x
  y_train = random_10_y
  
  remain_x = np.delete(copyX_class, random_indices, axis=0)
  remain_y = np.delete(copyY_class, random_indices, axis=0)
  
  scoreMeanList_ob, scoreList_un = activeLearning_hybrid_small(X_train, y_train, remain_x, remain_y)

  observedList_un_small.append(scoreMeanList_ob)
  unobservedList_un_small.append(scoreList_un)

print("obList:",np.array(observedList_un_small).shape)
print("Un obList:",np.array(unobservedList_un_small).shape)

# Offline learning (PCAWG dataset)

y_train = copy.deepcopy(label_codes)
X_train = copy.deepcopy(X)

model = RandomForestClassifier()
model.fit(X_train, y_train)
scoreA = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
scoreMean = np.mean(scoreA)
print(scoreMean)

# Offline learning (test dataset)
path3 = '/content/drive/MyDrive/Automation in Scientific Research/classification2.csv'

df3 = pd.read_csv(path3)
df3 = df3.drop(columns=df3.columns[-1])
#extract cost
cost = np.array(df3.iloc[:, -1])

#extract y
y3 = df3.iloc[:, 2]
y3 = ((np.array(y3))).flatten()

#extract x
x_class =[]
x1_c = np.array(df3.iloc[:,0].values)
x2_c = np.array(df3.iloc[:,1].values)
for i in range(x1_c.shape[0]):
  x_class.append([x1_c[i], x2_c[i]])
x_class = np.array(x_class)
copyX_class = copy.deepcopy(x_class)
copyY_class = copy.deepcopy(y3)
copy_cost = copy.deepcopy(cost)
print(copyX_class.shape)
print(copyY_class.shape)
print(cost.shape)

model = RandomForestClassifier()
model.fit(copyX_class, copyY_class)
scoreA = cross_val_score(model, copyX_class, copyY_class, cv=5, scoring='accuracy')
scoreMean = np.mean(scoreA)
print(scoreMean)

# Offline learning accuracies
offlineAcc_PCAWG = 0.7196388261851017
offlineAcc_test = 0.865

# Hybrid
# Seeds used (123, 124, 126, 127, 128)
# Initial accuracies
initAcc_ob = [0.7093073593073593, 0.6430166324903166, 0.6656641604010025, 0.691205285942128, 0.6957507404875826]
initAcc_un = [0.670535138620245, 0.6872985170857512, 0.68214055448098, 0.6653771760154739, 0.6782720825274017]

hybrid_sim1_ob = [0.7093073593073593, 0.7069314212171355, 0.7227528294998175, 0.7213405379151236, 0.7230343637378813, 0.7478983634797589, 0.7581306628516928, 0.7521816733067729, 0.7393441713366254, 0.7449259581881533, 0.7388704318936876, 0.7295238095238096, 0.7236784766089903, 0.7243662561782692, 0.7147567406851009, 0.7324396782841823, 0.7249012598572786, 0.7166307729121799, 0.7266789909952889, 0.7181053088351168, 0.7225780914998186, 0.7155963302752293]
hybrid_sim1_un = [0.670535138620245, 0.6623815967523681, 0.6457883369330454, 0.6518745218056619, 0.6399345335515548, 0.625, 0.6158245948522403, 0.5933609958506224, 0.6123853211009175, 0.6082474226804123, 0.6070422535211267, 0.628125, 0.6060606060606061, 0.6311475409836066, 0.6229508196721312, 0.6114285714285714, 0.5800711743772242, 0.6108597285067874, 0.6566265060240963, 0.6885245901639344, 0.654320987654321, 0.8]

hybrid_sim2_ob = [0.6430166324903166, 0.6565876612059951, 0.6926975287156283, 0.6855614973262033, 0.7144305998261373, 0.7273879062114357, 0.7178571428571429, 0.7293983607114919, 0.72, 0.7337497903739727, 0.7272809873551905, 0.7332495342416556, 0.7273723770748513, 0.7370482866043614, 0.7233673432468612, 0.7332985524528801, 0.7344835140760215, 0.733594180216213, 0.7208793562812119, 0.7311557788944724, 0.7285349520643638, 0.720377750625207, 0.7309199327803979, 0.7292237442922375]
hybrid_sim2_un = [0.6872985170857512, 0.6755829903978052, 0.6581446311176041, 0.6609375, 0.6362876254180602, 0.6264626462646264, 0.6107954545454546, 0.5972083748753739, 0.6106382978723405, 0.6036446469248291, 0.6232961586121437, 0.6224066390041494, 0.5982008995502249, 0.5830618892508144, 0.5869565217391305, 0.6033402922755741, 0.5751173708920188, 0.5574229691876751, 0.5845070422535211, 0.5688888888888889, 0.6235955056179775, 0.6097560975609756, 0.3880597014925373, 0.28]

hybrid_sim3_ob = [0.6656641604010025, 0.6803154409201813, 0.6888419273034657, 0.6949514116497039, 0.6979054726368159, 0.7139136684564326, 0.7261283869794508, 0.7393638542217796, 0.74926243567753, 0.7480376207422471, 0.7417790846445647, 0.7430379746835444, 0.7419508151423044, 0.7384164222873901, 0.7317392487316143, 0.7235892531084254, 0.726943005181347, 0.7148535992839792, 0.7180661945367828, 0.7213429256594723, 0.7217798594847775, 0.7225326979195096]
hybrid_sim3_un = [0.68214055448098, 0.6742112482853223, 0.6627822286962856, 0.6487538940809969, 0.6394389438943895, 0.6445035460992907, 0.62607861936721, 0.6080084299262382, 0.590347923681257, 0.601965601965602, 0.5953038674033149, 0.6, 0.5862676056338029, 0.5627450980392157, 0.546875, 0.5682451253481894, 0.5859649122807018, 0.6160337552742616, 0.6703910614525139, 0.6076923076923076, 0.55, 0.41935483870967744]

hybrid_sim4_ob = [0.691205285942128, 0.7191434468524251, 0.6968544159161179, 0.7211685060871544, 0.7357867049258727, 0.7335999676806851, 0.7426088822141171, 0.7603690418287938, 0.7461336010418926, 0.7405119089939566, 0.7491741268440297, 0.7403232087227414, 0.7500218184357055, 0.7490220418556537, 0.7517615176151762, 0.752494812493988, 0.7405680796931762, 0.7419261732959628, 0.7383452098178939, 0.7325030076480192, 0.7197278911564626]
hybrid_sim4_un = [0.6653771760154739, 0.6668960770818995, 0.6558345642540621, 0.6512900703674745, 0.6236378876781223, 0.6259057971014492, 0.6025145067698259, 0.569593147751606, 0.5981308411214953, 0.5918635170603674, 0.6160714285714286, 0.5938009787928222, 0.5277246653919694, 0.504524886877828, 0.5026737967914439, 0.512987012987013, 0.4469026548672566, 0.46153846153846156, 0.37168141592920356, 0.3448275862068966, 0.2]

hybrid_sim5_ob = [0.6957507404875826, 0.7087682119205297, 0.7183431952662722, 0.7365533275210694, 0.7385413677666188, 0.7468729980781551, 0.7477230214603953, 0.7353846153846153, 0.741597818240154, 0.7527397260273974, 0.7440452235245306, 0.7486164259754245, 0.7454076586124063, 0.7447293447293447, 0.7415300546448088, 0.7275299714297432, 0.7287026922829789, 0.7302914808955367, 0.730391358443615, 0.7244929964767551, 0.728142472561607]
hybrid_sim5_un = [0.6782720825274017, 0.6691729323308271, 0.656934306569343, 0.65244375484872, 0.6348408710217756, 0.6298997265268915, 0.6197604790419161, 0.6120218579234973, 0.6018845700824499, 0.6198675496688741, 0.625, 0.6258278145695364, 0.5880149812734082, 0.5673913043478261, 0.5662337662337662, 0.5841584158415841, 0.5473251028806584, 0.4941860465116279, 0.5178571428571429, 0.576271186440678, 0.15789473684210525]

# QBC
query_sim1_ob = [0.7093073593073593, 0.6972431077694236, 0.6927660059239007, 0.6972658920027341, 0.6972544998860789, 0.6912622465254044, 0.6987468671679198, 0.7048074732285258, 0.70477329687856, 0.6911938938254727, 0.6927432216905901, 0.7062542720437458, 0.6897243107769423, 0.703280929596719, 0.6942014126224653, 0.6942583732057416]
query_sim1_un = [0.670535138620245, 0.6671261199172984, 0.6439674315321984, 0.6338928856914469, 0.6264118158123371, 0.6289248334919124, 0.6267087276550999, 0.6098707403055229, 0.5858854860186418, 0.5683563748079877, 0.5499092558983666, 0.5476718403547672, 0.5498575498575499, 0.49800796812749004, 0.37748344370860926, 0.37254901960784315]

query_sim2_ob = [0.6430166324903166, 0.64004329004329, 0.650580997949419, 0.6370471633629529, 0.6340396445659603, 0.6415242652084757, 0.6279904306220095, 0.6400318979266347, 0.6355434039644565, 0.646058327637275, 0.6400205058099795, 0.6400205058099796, 0.643028024606972, 0.6430508088402827, 0.6400091136933241, 0.6309865573023468]
query_sim2_un = [0.6872985170857512, 0.6726395589248794, 0.6454478164322723, 0.627498001598721, 0.6099044309296264, 0.6070409134157945, 0.6203995793901157, 0.6286721504112809, 0.6218375499334221, 0.6129032258064516, 0.5989110707803993, 0.5898004434589801, 0.5925925925925926, 0.5617529880478087, 0.3509933774834437, 0.3333333333333333]

query_sim3_ob = [0.6656641604010025, 0.6611414900888585, 0.6671679197994986, 0.6717019822282979, 0.6746411483253588, 0.6747322852586011, 0.6656983367509683, 0.6746981089086351, 0.6626908179539759, 0.6656527682843473, 0.6686944634313056, 0.6596491228070176, 0.6732171337434496, 0.6671679197994986, 0.6626794258373205, 0.671701982228298]
query_sim3_un = [0.68214055448098, 0.658855961405927, 0.6617320503330866, 0.657074340527578, 0.6437880104257168, 0.6527117031398668, 0.6267087276550999, 0.6380728554641598, 0.6151797603195739, 0.620583717357911, 0.5989110707803993, 0.5875831485587583, 0.5897435897435898, 0.5179282868525896, 0.37748344370860926, 0.39215686274509803]

query_sim4_ob = [0.691205285942128, 0.6881863750284803, 0.7047505126452495, 0.7077808156755524, 0.6987240829346092, 0.6927204374572795, 0.6972317156527683, 0.6957051720209615, 0.6911825017088175, 0.7108111187058557, 0.6927090453406243, 0.6927090453406242, 0.7047505126452495, 0.7077238550922761, 0.6942014126224653, 0.7002278423331054]
query_sim4_un = [0.6653771760154739, 0.6629910406616126, 0.6424870466321243, 0.6338928856914469, 0.6090356211989574, 0.6279733587059942, 0.6256572029442692, 0.6321974148061105, 0.6378162450066578, 0.6098310291858678, 0.588021778584392, 0.5521064301552107, 0.5213675213675214, 0.450199203187251, 0.31788079470198677, 0.39215686274509803]

query_sim5_ob = [0.6957507404875826, 0.6791524265208475, 0.6806675780359991, 0.6791979949874686, 0.6836978810663021, 0.6882205513784461, 0.6776714513556619, 0.6837092731829573, 0.6836864889496468, 0.6821941216678059, 0.6716450216450216, 0.6836864889496468, 0.6836637047163363, 0.6746297562087036, 0.6761562998405104, 0.6746639325586694]
query_sim5_un = [0.6782720825274017, 0.6691936595451413, 0.6602516654330126, 0.6402877697841727, 0.6220677671589921, 0.6336822074215034, 0.6235541535226078, 0.6380728554641598, 0.6418109187749668, 0.6082949308755761, 0.588021778584392, 0.532150776053215, 0.4843304843304843, 0.4342629482071713, 0.31125827814569534, 0.3137254901960784]

# Uncertainty
uncert_sim1_ob = [0.7093073593073593, 0.7081613347093223, 0.711816104315096, 0.7375701640759931, 0.7518779342723005, 0.749134231167678, 0.7594955768868812, 0.7690583925878044, 0.7650170648464164, 0.7806832145490292, 0.7656156156156156, 0.7641530388874582, 0.756422785320995, 0.7423573246092331, 0.7325524812524977, 0.7407460867333847, 0.728216704288939]
uncert_sim1_un = [0.670535138620245, 0.6581667815299793, 0.6321243523316062, 0.6131095123900879, 0.5960034752389227, 0.5756422454804948, 0.583596214511041, 0.5710928319623971, 0.511318242343542, 0.4500768049155146, 0.4337568058076225, 0.45454545454545453, 0.48717948717948717, 0.5258964143426295, 0.5165562913907285, 0.1568627450980392]

uncert_sim2_ob = [0.6430166324903166, 0.6805813553491572, 0.6944145718510553, 0.7209358808290156, 0.7386881034635485, 0.7379273346159538, 0.7420697659828095, 0.7551066580478345, 0.7588620318855487, 0.7640554599819775, 0.753003003003003, 0.7652990600051506, 0.7451656144599152, 0.7464311678870021, 0.7335221796469122, 0.7384441023009153, 0.725056433408578]
uncert_sim2_un = [0.6872985170857512, 0.6602343211578222, 0.6350851221317543, 0.6187050359712231, 0.5925282363162467, 0.5889628924833492, 0.594111461619348, 0.5898942420681551, 0.5339547270306259, 0.4823348694316436, 0.4664246823956443, 0.4678492239467849, 0.5156695156695157, 0.5258964143426295, 0.5099337748344371, 0.23529411764705882]

uncert_sim3_ob = [0.6656641604010025, 0.676702786377709, 0.6967603172469418, 0.7251025474956821, 0.7519089379041545, 0.7482869616693799, 0.7515998494259364, 0.7544117647058823, 0.7684650988826032, 0.762795936757598, 0.7608216650385324, 0.7573847540561423, 0.7371270431548906, 0.7398024095134236, 0.7267436940219563, 0.7231994696775297, 0.7237020316027087]
uncert_sim3_un = [0.68214055448098, 0.6616126809097175, 0.6432272390821614, 0.6203037569944044, 0.6046915725456126, 0.5727878211227403, 0.5856992639327024, 0.5887191539365453, 0.5512649800266312, 0.533026113671275, 0.4863883847549909, 0.47671840354767187, 0.5327635327635327, 0.5657370517928287, 0.6688741721854304, 0.7450980392156863]

uncert_sim4_ob = [0.691205285942128, 0.7093567251461989, 0.7291302594434736, 0.7437661917098446, 0.7631455399061033, 0.769749889003996, 0.7618671183888576, 0.7698125404007756, 0.7780097246248071, 0.7838883427541574, 0.7662270704439379, 0.7675766160185423, 0.7574966127590879, 0.7525497221789478, 0.7412856437621945, 0.7421435292105037, 0.7273137697516929]
uncert_sim4_un = [0.6653771760154739, 0.6409372846312887, 0.6365655070318282, 0.6187050359712231, 0.5977410947002606, 0.5756422454804948, 0.5636172450052577, 0.564042303172738, 0.5552596537949401, 0.48847926267281105, 0.47005444646098005, 0.4523281596452328, 0.49002849002849, 0.4860557768924303, 0.44370860927152317, 0.13725490196078433]

uncert_sim5_ob = [0.6957507404875826, 0.7146370829033367, 0.730340099475736, 0.7427623056994819, 0.7631854017184869, 0.764618173745745, 0.7468379446640315, 0.7624703727644904, 0.7786853055308803, 0.7749426558531989, 0.7572090162451609, 0.7641852304918878, 0.7580529850961399, 0.7550903567533884, 0.7422494651966431, 0.7458440253186212, 0.7354401805869075]
uncert_sim5_un = [0.6782720825274017, 0.6643694004135079, 0.6439674315321984, 0.6346922462030375, 0.6003475238922676, 0.5803996194100857, 0.594111461619348, 0.5734430082256169, 0.5406125166444741, 0.5023041474654378, 0.5009074410163339, 0.48337028824833705, 0.5014245014245015, 0.5099601593625498, 0.5033112582781457, 0.11764705882352941]

# Diversity
diversity_sim1_ob = [0.7093073593073593, 0.6896972824217407, 0.6735448312945289, 0.6669797063903282, 0.6794888829834352, 0.6950495782151843, 0.7025566221218396, 0.7030839258780435, 0.7014984337743699, 0.703325960514459, 0.7097181518868266, 0.7136894797836725, 0.7097379572775232, 0.710256010801267, 0.7204247866663532, 0.7208611324950818]
diversity_sim1_un = [0.670535138620245, 0.6747070985527223, 0.6980014803849001, 0.689048760991207, 0.6941789748045178, 0.69267364414843, 0.7024185068349106, 0.699177438307873, 0.7030625832223701, 0.6989247311827957, 0.7168784029038112, 0.7272727272727273, 0.7122507122507122, 0.7211155378486056, 0.6622516556291391, 0.7450980392156863]

diversity_sim2_ob = [0.6430166324903166, 0.6505073959408324, 0.6585764215620379, 0.6836193868739205, 0.6701036407121977, 0.674385822110404, 0.6922579835623315, 0.703110859728507, 0.7090443686006827, 0.709723928893258, 0.7073302217880532, 0.7086144733453515, 0.7145737841967195, 0.7138300358311263, 0.718970826770728, 0.7213337182448037]
diversity_sim2_un = [0.6872985170857512, 0.6953824948311509, 0.7128053293856402, 0.7074340527577938, 0.7324066029539531, 0.7345385347288297, 0.732912723449001, 0.7320799059929495, 0.7310252996005326, 0.7403993855606759, 0.7386569872958257, 0.7405764966740577, 0.7350427350427351, 0.7410358565737052, 0.695364238410596, 0.7254901960784313]

diversity_sim3_ob = [0.6656641604010025, 0.6622635018919848, 0.6550544428014519, 0.6639140759930916, 0.6701169279829923, 0.6812601746337132, 0.6875180375180376, 0.6898917259211377, 0.6987750712983309, 0.7033075284672728, 0.7055410832519267, 0.7074765001287664, 0.7060047853786504, 0.7107804954042685, 0.7175274454030418, 0.7176374989308014]
diversity_sim3_un = [0.68214055448098, 0.6912474155754652, 0.7120651369356032, 0.706634692246203, 0.7219808861859253, 0.7183634633682208, 0.7139852786540484, 0.7191539365452408, 0.729693741677763, 0.7511520737327189, 0.7441016333938294, 0.7583148558758315, 0.7464387464387464, 0.7290836653386454, 0.7019867549668874, 0.7647058823529411]

diversity_sim4_ob = [0.691205285942128, 0.6923374613003096, 0.682833714208899, 0.6867012089810017, 0.689839666932412, 0.690720734053574, 0.6914580588493632, 0.6994370825253178, 0.6912618635747347, 0.7058982550995331, 0.7031332537356633, 0.7137136234869946, 0.7070656404047392, 0.7219777223866646, 0.711219116575378, 0.7162550252330854]
diversity_sim4_un = [0.6653771760154739, 0.684355616815989, 0.6861584011843079, 0.697841726618705, 0.6915725456125109, 0.7040913415794482, 0.7034700315457413, 0.7121034077555817, 0.725699067909454, 0.7204301075268817, 0.7223230490018149, 0.7161862527716186, 0.7122507122507122, 0.7211155378486056, 0.695364238410596, 0.7843137254901961]

diversity_sim5_ob = [0.6957507404875826, 0.6832129342965256, 0.6620311869874983, 0.6732620898100172, 0.673850651076269, 0.6907540328548172, 0.6954263128176172, 0.6979476405946995, 0.703550890644724, 0.7077988039649382, 0.7085314229892543, 0.7159943986608293, 0.7178111216812246, 0.7077322532066261, 0.7180152330802323, 0.7120990505517064]
diversity_sim5_un = [0.6782720825274017, 0.6940041350792557, 0.7046632124352331, 0.7082334132693845, 0.7167680278019114, 0.7069457659372027, 0.7129337539432177, 0.7191539365452408, 0.7403462050599201, 0.7419354838709677, 0.7404718693284936, 0.7272727272727273, 0.7264957264957265, 0.7250996015936255, 0.695364238410596, 0.7843137254901961]

# Hybrid on test dataset
# Seeds used: 123, 124, 125, 126, 127
initTestAcc_ob = [0.85, 0.9333333333333332, 0.85, 0.8666666666666666, 0.8666666666666668]
initTestAcc_un = [0.8142857142857143, 0.7714285714285715, 0.8214285714285714, 0.8285714285714286, 0.8428571428571429]

test_sim1_ob = [0.85, 0.8714285714285713, 0.875, 0.8777777777777779, 0.89, 0.9090909090909092, 0.906521739130435, 0.9301538461538461, 0.942063492063492, 0.9317241379310344, 0.9487903225806452, 0.9342245989304814, 0.9265079365079366, 0.9237237237237238, 0.9425101214574898]
test_sim1_un = [0.8142857142857143, 0.8461538461538461, 0.85, 0.8363636363636363, 0.89, 0.9, 0.9259259259259259, 0.9436619718309859, 0.9354838709677419, 0.9423076923076923, 0.9069767441860465, 0.8787878787878788, 0.9130434782608695, 0.9375, 1.0]

test_sim2_ob = [0.9333333333333332, 0.8857142857142858, 0.8833333333333334, 0.9196078431372549, 0.9378947368421052, 0.9354978354978355, 0.9326086956521739, 0.9212307692307693, 0.8243386243386244, 0.8841379310344827, 0.904233870967742, 0.9212121212121213, 0.9028571428571428, 0.9076576576576578, 0.8894736842105264, 0.9180769230769229]
test_sim2_un = [0.7714285714285715, 0.7846153846153846, 0.8016528925619835, 0.8125, 0.7647058823529411, 0.8586956521739131, 0.8048780487804879, 0.8493150684931506, 0.873015873015873, 0.8867924528301887, 0.813953488372093, 0.8285714285714286, 0.92, 1.0, 1.0, 1.0]

test_sim3_ob = [0.85, 0.8142857142857143, 0.8, 0.8542483660130717, 0.8447368421052632, 0.8779220779220779, 0.8754940711462451, 0.9179999999999999, 0.8490028490028492, 0.8369458128078817, 0.8610752688172043, 0.8820075757575758, 0.9005042016806722, 0.9277777777777777, 0.926031294452347, 0.923076923076923]
test_sim3_un = [0.8214285714285714, 0.823076923076923, 0.8083333333333333, 0.8108108108108109, 0.8349514563106796, 0.8404255319148937, 0.9069767441860465, 0.9090909090909091, 0.9552238805970149, 0.9661016949152542, 0.9591836734693877, 0.9230769230769231, 0.9310344827586207, 0.95, 0.9090909090909091, 1.0]

test_sim4_ob = [0.8666666666666666, 0.8978021978021978, 0.8833333333333334, 0.9176470588235294, 0.9052631578947368, 0.8761904761904763, 0.9391304347826086, 0.9349999999999999, 0.9088319088319088, 0.9285714285714286, 0.9533333333333334, 0.9366935483870968, 0.9584670231729054, 0.9492063492063492, 0.9513513513513514, 0.9426450742240216, 0.9387179487179488]
test_sim4_un = [0.8285714285714286, 0.8549618320610687, 0.8943089430894309, 0.9035087719298246, 0.8867924528301887, 0.8541666666666666, 0.8255813953488372, 0.8333333333333334, 0.8970588235294118, 0.8983050847457628, 0.88, 0.8333333333333334, 0.8125, 0.782608695652174, 0.8, 0.625, 0.75]

test_sim5_ob = [0.8666666666666668, 0.8714285714285716, 0.9125, 0.9222222222222222, 0.8994736842105263, 0.9246753246753245, 0.922463768115942, 0.9199999999999999, 0.9037037037037037, 0.9165024630541871, 0.9094623655913978, 0.9625, 0.9352941176470587, 0.9555555555555555, 0.9520625889046942, 0.9589743589743591]
test_sim5_un = [0.8428571428571429, 0.8461538461538461, 0.8264462809917356, 0.8738738738738738, 0.8910891089108911, 0.8723404255319149, 0.9047619047619048, 0.88, 0.8615384615384616, 0.875, 0.8695652173913043, 0.8205128205128205, 0.8, 0.8, 0.8181818181818182, 0.2]

# Graph PCAWG results
hybridAcc_ob = []
hybridAcc_un = []
hybridAcc_ob.append(hybrid_sim1_ob)
hybridAcc_ob.append(hybrid_sim2_ob)
hybridAcc_ob.append(hybrid_sim3_ob)
hybridAcc_ob.append(hybrid_sim4_ob)
hybridAcc_ob.append(hybrid_sim5_ob)
hybridAcc_un.append(hybrid_sim1_un)
hybridAcc_un.append(hybrid_sim2_un)
hybridAcc_un.append(hybrid_sim3_un)
hybridAcc_un.append(hybrid_sim4_un)
hybridAcc_un.append(hybrid_sim5_un)

queryAcc_ob = []
queryAcc_un = []
queryAcc_ob.append(query_sim1_ob)
queryAcc_ob.append(query_sim2_ob)
queryAcc_ob.append(query_sim3_ob)
queryAcc_ob.append(query_sim4_ob)
queryAcc_ob.append(query_sim5_ob)
queryAcc_un.append(query_sim1_un)
queryAcc_un.append(query_sim2_un)
queryAcc_un.append(query_sim3_un)
queryAcc_un.append(query_sim4_un)
queryAcc_un.append(query_sim5_un)

uncertAcc_ob = []
uncertAcc_un = []
uncertAcc_ob.append(uncert_sim1_ob)
uncertAcc_ob.append(uncert_sim2_ob)
uncertAcc_ob.append(uncert_sim3_ob)
uncertAcc_ob.append(uncert_sim4_ob)
uncertAcc_ob.append(uncert_sim5_ob)
uncertAcc_un.append(uncert_sim1_un)
uncertAcc_un.append(uncert_sim2_un)
uncertAcc_un.append(uncert_sim3_un)
uncertAcc_un.append(uncert_sim4_un)
uncertAcc_un.append(uncert_sim5_un)

diversityAcc_ob = []
diversityAcc_un = []
diversityAcc_ob.append(diversity_sim1_ob)
diversityAcc_ob.append(diversity_sim2_ob)
diversityAcc_ob.append(diversity_sim3_ob)
diversityAcc_ob.append(diversity_sim4_ob)
diversityAcc_ob.append(diversity_sim5_ob)
diversityAcc_un.append(diversity_sim1_un)
diversityAcc_un.append(diversity_sim2_un)
diversityAcc_un.append(diversity_sim3_un)
diversityAcc_un.append(diversity_sim4_un)
diversityAcc_un.append(diversity_sim5_un)

# Make all simulations the same length within one method (append zeroes to make same length)
hybridAcc_lengths_ob = [len(i) for i in hybridAcc_ob]
maxHybridLengthOb = max(hybridAcc_lengths_ob)
for l in hybridAcc_ob:
  if len(l) < maxHybridLengthOb:
    for i in range(maxHybridLengthOb - len(l)):
      l.append(0)
hybridAcc_lengths_un = [len(i) for i in hybridAcc_un]
maxHybridLengthUn = max(hybridAcc_lengths_un)
for l in hybridAcc_un:
  if len(l) < maxHybridLengthUn:
    for i in range(maxHybridLengthUn - len(l)):
      l.append(0)

queryAcc_lengths_ob = [len(i) for i in queryAcc_ob]
maxQueryLengthOb = max(queryAcc_lengths_ob)
for l in queryAcc_ob:
  if len(l) < maxQueryLengthOb:
    for i in range(maxQueryLengthOb - len(l)):
      l.append(0)
queryAcc_lengths_un = [len(i) for i in queryAcc_un]
maxQueryLengthUn = max(queryAcc_lengths_un)
for l in queryAcc_un:
  if len(l) < maxQueryLengthUn:
    for i in range(maxQueryLengthUn - len(l)):
      l.append(0)

uncertAcc_lengths_ob = [len(i) for i in uncertAcc_ob]
maxUncertLengthOb = max(uncertAcc_lengths_ob)
for l in uncertAcc_ob:
  if len(l) < maxUncertLengthOb:
    for i in range(maxUncertLengthOb - len(l)):
      l.append(0)
uncertAcc_lengths_un = [len(i) for i in uncertAcc_un]
maxUncertLengthUn = max(uncertAcc_lengths_un)
for l in uncertAcc_un:
  if len(l) < maxUncertLengthUn:
    for i in range(maxUncertLengthUn - len(l)):
      l.append(0)

diversityAcc_lengths_ob = [len(i) for i in diversityAcc_ob]
maxDiversityLengthOb = max(diversityAcc_lengths_ob)
for l in diversityAcc_ob:
  if len(l) < maxDiversityLengthOb:
    for i in range(maxDiversityLengthOb - len(l)):
      l.append(0)
diversityAcc_lengths_un = [len(i) for i in diversityAcc_un]
maxDiversityLengthUn = max(diversityAcc_lengths_un)
for l in diversityAcc_un:
  if len(l) < maxDiversityLengthUn:
    for i in range(maxDiversityLengthUn - len(l)):
      l.append(0)

import statistics

avgHybrid = []
stdHybrid = []
avgQuery = []
stdQuery = []
avgUncert = []
stdUncert = []
avgDiversity = []
stdDiversity = []
numRoundsHybrid = len(hybridAcc_ob[0])
numRoundsQuery = len(queryAcc_ob[0])
numRoundsUncert = len(uncertAcc_ob[0])
numRoundsDiversity = len(diversityAcc_ob[0])

for i in range(numRoundsHybrid):
  roundHybrid = []
  for j in range(5):
    roundHybrid.append(hybridAcc_ob[j][i])
  roundAvgHybrid = sum(roundHybrid) / len(roundHybrid)
  avgHybrid.append(roundAvgHybrid)
  stdHybrid.append(statistics.stdev(roundHybrid))

for i in range(numRoundsQuery):
  roundQuery = []
  for j in range(5):
    roundQuery.append(queryAcc_ob[j][i])
  roundAvgQuery = sum(roundQuery) / len(roundQuery)
  avgQuery.append(roundAvgQuery)
  stdQuery.append(statistics.stdev(roundQuery))

for i in range(numRoundsUncert):
  roundUncert = []
  for j in range(5):
    roundUncert.append(uncertAcc_ob[j][i])
  roundAvgUncert = sum(roundUncert) / len(roundUncert)
  avgUncert.append(roundAvgUncert)
  stdUncert.append(statistics.stdev(roundUncert))

for i in range(numRoundsDiversity):
  roundDiversity = []
  for j in range(5):
    roundDiversity.append(diversityAcc_ob[j][i])
  roundAvgDiversity = sum(roundDiversity) / len(roundDiversity)
  avgDiversity.append(roundAvgDiversity)
  stdDiversity.append(statistics.stdev(roundDiversity))

plt.errorbar(np.arange(0, maxHybridLengthOb - 4), avgHybrid[:-4], yerr = stdHybrid[:-4], label = 'Hybrid')
plt.errorbar(np.arange(0, maxQueryLengthOb), avgQuery, yerr = stdQuery, label = 'QBC')
plt.errorbar(np.arange(0, maxUncertLengthOb - 1), avgUncert[:-1], yerr = stdUncert[:-1], label = 'Uncertainty')
plt.errorbar(np.arange(0, maxDiversityLengthOb), avgDiversity, yerr = stdDiversity, label = 'Diversity')
plt.xticks(np.arange(0, 20, 1))
plt.axhline(xmin = 0.046, xmax = 0.955, y = 0.7196388261851017, color = 'purple', label = 'Offline')
plt.xlabel('Iteration Number')
plt.ylabel('Classification Accuracy')
plt.title('Performance on Observed Set with Active Learning Methods')
plt.legend()
plt.show()

# Graph PCAWG results (unobserved)
avgHybrid = []
stdHybrid = []
avgQuery = []
stdQuery = []
avgUncert = []
stdUncert = []
avgDiversity = []
stdDiversity = []
numRoundsHybrid = len(hybridAcc_un[0])
numRoundsQuery = len(queryAcc_un[0])
numRoundsUncert = len(uncertAcc_un[0])
numRoundsDiversity = len(diversityAcc_un[0])

for i in range(numRoundsHybrid):
  roundHybrid = []
  for j in range(5):
    roundHybrid.append(hybridAcc_un[j][i])
  roundAvgHybrid = sum(roundHybrid) / len(roundHybrid)
  avgHybrid.append(roundAvgHybrid)
  stdHybrid.append(statistics.stdev(roundHybrid))

for i in range(numRoundsQuery):
  roundQuery = []
  for j in range(5):
    roundQuery.append(queryAcc_un[j][i])
  roundAvgQuery = sum(roundQuery) / len(roundQuery)
  avgQuery.append(roundAvgQuery)
  stdQuery.append(statistics.stdev(roundQuery))

for i in range(numRoundsUncert):
  roundUncert = []
  for j in range(5):
    roundUncert.append(uncertAcc_un[j][i])
  roundAvgUncert = sum(roundUncert) / len(roundUncert)
  avgUncert.append(roundAvgUncert)
  stdUncert.append(statistics.stdev(roundUncert))

for i in range(numRoundsDiversity):
  roundDiversity = []
  for j in range(5):
    roundDiversity.append(diversityAcc_un[j][i])
  roundAvgDiversity = sum(roundDiversity) / len(roundDiversity)
  avgDiversity.append(roundAvgDiversity)
  stdDiversity.append(statistics.stdev(roundDiversity))

plt.errorbar(np.arange(0, maxHybridLengthUn - 4), avgHybrid[:-4], yerr = stdHybrid[:-4], label = 'Hybrid')
plt.errorbar(np.arange(0, maxQueryLengthUn), avgQuery, yerr = stdQuery, label = 'QBC')
plt.errorbar(np.arange(0, maxUncertLengthUn), avgUncert, yerr = stdUncert, label = 'Uncertainty')
plt.errorbar(np.arange(0, maxDiversityLengthUn), avgDiversity, yerr = stdDiversity, label = 'Diversity')
plt.xticks(np.arange(0, 20, 1))
plt.xlabel('Iteration Number')
plt.ylabel('Classification Accuracy')
plt.title('Performance on Unobserved Set with Active Learning Methods')
plt.legend()
plt.show()

# Graph test dataset results
testAcc_ob = []
testAcc_un = []
testAcc_ob.append(test_sim1_ob)
testAcc_ob.append(test_sim2_ob)
testAcc_ob.append(test_sim3_ob)
testAcc_ob.append(test_sim4_ob)
testAcc_ob.append(test_sim5_ob)
testAcc_un.append(test_sim1_un)
testAcc_un.append(test_sim2_un)
testAcc_un.append(test_sim3_un)
testAcc_un.append(test_sim4_un)
testAcc_un.append(test_sim5_un)

avgOb = []
stdOb = []
avgUn = []
stdUn = []
numRounds = len(testAcc_ob[0])

for i in range(numRounds):
  roundOb = []
  for j in range(5):
    roundOb.append(testAcc_ob[j][i])
  roundAvgOb = sum(roundOb) / len(roundOb)
  avgOb.append(roundAvgOb)
  stdOb.append(statistics.stdev(roundOb))

for i in range(numRounds):
  roundUn = []
  for j in range(5):
    roundUn.append(testAcc_un[j][i])
  roundAvgUn = sum(roundUn) / len(roundUn)
  avgUn.append(roundAvgUn)
  stdUn.append(statistics.stdev(roundUn))

plt.errorbar(np.arange(numRounds), avgOb, yerr = stdOb, label = 'Observed')
plt.errorbar(np.arange(numRounds), avgUn, yerr = stdUn, label = 'Unobserved')
plt.axhline(xmin = 0.046, xmax = 0.955, y = 0.865, color = 'purple', label = 'Offline')
plt.xlabel('Iteration Number')
plt.ylabel('Classification Accuracy')
plt.title('Performance on Test Dataset with Hybrid Learning')
plt.legend()
plt.show()